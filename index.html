<!DOCTYPE html>
<html>
<head>
<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-81D6829LG0"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-81D6829LG0');
</script>

  <meta name="description" content="UIL-AQA is a novel uncertainty-aware, interpretable long-term Action Quality Assessment framework published in the International Journal of Computer Vision (IJCV), 2025. The model introduces Gaussian noise injection for uncertainty modelling, an attention-guided mechanism to prevent temporal skipping, and a difficulty–quality regression head enabling clip-level interpretability.">
<meta property="og:title" content="UIL-AQA: Uncertainty-aware Clip-level Interpretable Action Quality Assessment"/>
<meta property="og:description" content="UIL-AQA introduces uncertainty-aware modelling and interpretable clip-level difficulty–quality scoring for long-term Action Quality Assessment. Published in IJCV 2025."/>
<meta property="og:url" content="https://andrewjohngilbert.github.io/UILAQA/"/>
<meta property="og:image" content="assets/UIL-AQA_Teaser.jpg" />

<meta name="twitter:title" content="UIL-AQA: Uncertainty-aware Clip-level Interpretable Action Quality Assessment">
<meta name="twitter:description" content="Published in IJCV 2025, UIL-AQA introduces uncertainty-aware modelling and interpretable difficulty–quality scoring for long-term AQA.">
<meta name="twitter:image" content="assets/UIL-AQA_Teaser.jpg">
<meta name="twitter:card" content="summary_large_image">

<meta name="keywords" content="Action Quality Assessment, Uncertainty Modelling, Interpretability, Transformer, IJCV 2025, Andrew Gilbert">



  <title>UIL-AQA: Uncertainty-aware Clip-level Interpretable Action Quality Assessment</title>
  <link rel="icon" type="image/x-icon" href="static/images/favicon-32x32.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">
  UIL-AQA: Uncertainty-aware Clip-level Interpretable Action Quality Assessment
</h1>
            <div class="is-size-5 publication-authors">
  <span class="author-block"><a href="https://www.linkedin.com/in/xudong-442302166/" target="_blank">Xu Dong</a>[1],</span>
  <span class="author-block"><a href="https://www.surrey.ac.uk/people/xinran-liu" target="_blank">Xinran Liu</a>[1],</span>
  <span class="author-block"><a href="https://scholars.uow.edu.au/wanqing-li" target="_blank">Wanqing Li</a>[2],</span>
  <span class="author-block"><a href="https://www.surrey.ac.uk/people/anthony-adeyemi-ejeye" target="_blank">Anthony Adeyemi-Ejeye</a>[1],</span>
  <span class="author-block"><a href="https://andrewjohngilbert.github.io/" target="_blank">Andrew Gilbert</a>[1]</span>
</div>

<div class="is-size-5 publication-authors">
  <span class="author-block">[1] University of Surrey &nbsp; [2] University of Wollongong<br>
  <em>International Journal of Computer Vision (IJCV), 2025</em>
</div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="assets/IJCV_editor_verison.pdf" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

 
                  <!-- Github link -->
                   <!--
                <span class="link-block">
                    <a href="https://github.com/SadeghRahmaniB/DEAR" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

              -->
              

                
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser hero-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="assets/UIL-AQA_Teaser.jpg" alt="UIL-AQA teaser figure">
      <h2 class="subtitle has-text-centered">
        UIL-AQA is a long-term Action Quality Assessment framework that regresses clip-level difficulty and quality while explicitly modelling judge uncertainty. A query-based Transformer decoder, an attention loss to prevent temporal skipping, and a Gaussian noise injection module together yield interpretable clip-wise scores and robust predictions across rhythmic gymnastics, figure skating, and group performance datasets.
      </h2>
    </div>
  </div>
</section>
<!-- End teaser hero -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Action Quality Assessment (AQA) aims to score how well an action is performed in a video, but long-term sequences, subjective judging and a single global score make this problem challenging. Existing methods often focus on short clips, lack clip-level interpretability, and ignore uncertainty arising from human bias. In this work we propose UIL-AQA, a query-based Transformer framework designed to be both clip-level interpretable and uncertainty-aware for long-term AQA. The model introduces an Attention Loss and a Query Initialization module to mitigate a phenomenon we term <em>Temporal Skipping</em>, where self-attention layers are effectively bypassed, weakening temporal modelling. We further add a Gaussian Noise Injection module that simulates variability in human scoring, improving robustness to subjective and noisy labels. Finally, a Difficulty–Quality Regression Head decomposes each clip’s contribution into separate difficulty and quality components, enabling fine-grained, human-aligned analysis of performances. Experiments on three long-term AQA benchmarks – Rhythmic Gymnastics (RG), Figure Skating Video (Fis-V), and LOng-form GrOup (LOGO) – show that UIL-AQA achieves state-of-the-art performance while providing more interpretable clip-wise scores.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->



<!-- System diagram-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <h2 class="title is-3">Overview of the UIL-AQA architecture</h2>
      <img src="assets/UIL-AQA_System.jpg" alt="UIL-AQA System figure">
            <h2 class="subtitle has-text-centered">
        The input video is divided into clips and passed through a frozen feature extractor (I3D or Video Swin Transformer). A DETR-style Transformer decoder with learnable queries models long-range temporal structure. Attention Loss and Query Initialization mitigate <strong>Temporal Skipping</strong> by encouraging strong inter-query correlations. The Difficulty–Quality Regression Head outputs clip-level difficulty weights and quality scores, while the Gaussian Noise Injection module models uncertainty in human judgement and improves robustness.
            </em>
      </h2>
    </div>
  </div>
</section>
<!-- System diagram -->

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Method Overview</h2>

        <div class="content has-text-justified">

          <p>
            UIL-AQA is designed to provide <strong>interpretable</strong> and <strong>uncertainty-aware</strong> scoring for long-term Action Quality Assessment (AQA). Unlike prior methods that produce a single global score, UIL-AQA predicts <em>per-clip difficulty and quality</em>, while also modelling the uncertainty in human judgement. The framework consists of four key components:
          </p>

          <h3 class="title is-5">1. Feature Extraction</h3>
          <p>
            The input video is divided into fixed-length clips. Each clip is processed using a pretrained backbone (I3D or Video Swin Transformer) to extract spatiotemporal features. These features are frozen during training to ensure stability and efficiency.
          </p>

          <h3 class="title is-5">2. Temporal Decoder with Learnable Queries</h3>
          <p>
            UIL-AQA uses a <strong>DETR-style Transformer decoder</strong> equipped with learnable queries. Each query corresponds to a video clip and interacts with clip features through self-attention and cross-attention layers to capture long-range temporal structure. To prevent loss of temporal information, we introduce two innovations:
          </p>
          <ul>
            <li><strong>Attention Loss</strong> – Encourages alignment between self-attention and cross-attention maps, mitigating a failure mode we call <em>Temporal Skipping</em>.</li>
            <li><strong>Query Initialization</strong> – High-variance Gaussian initialisation promotes diverse temporal representations, strengthening inter-query relationships.</li>
          </ul>

          <h3 class="title is-5">3. Difficulty–Quality Regression Head</h3>
          <p>
            Instead of predicting only a single score, UIL-AQA regresses:
          </p>
          <ul>
            <li><strong>Difficulty</strong> – A softmax-normalised weight representing the relative contribution of each clip.</li>
            <li><strong>Quality</strong> – A clip-level quality score predicted independently for each segment.</li>
          </ul>
          <p>
            The final video score is obtained via a weighted sum of clip quality scores. This mirrors human judging processes in sports such as rhythmic gymnastics and figure skating, enabling <strong>fine-grained interpretability</strong>.
          </p>

          <h3 class="title is-5">4. Gaussian Noise Injection for Uncertainty Modelling</h3>
          <p>
            Human scoring often includes subjective variation. UIL-AQA introduces a <strong>Gaussian Noise Injection module</strong> during training to simulate this variability by perturbing predicted scores. This encourages the model to learn uncertainty-aware representations, improving robustness against noisy labels and inconsistent judge behaviour.
          </p>

          <h3 class="title is-5">Summary</h3>
          <p>
            Together, these components allow UIL-AQA to achieve <strong>state-of-the-art performance</strong> on three challenging long-form AQA benchmarks (RG, Fis-V, LOGO), while providing the rare ability to both <strong>explain its scoring</strong> and <strong>model uncertainty</strong> — two critical requirements for real-world deployment.
          </p>

        </div>
      </div>
    </div>
  </div>
</section>




<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3">Results</h2>

    <div class="content has-text-justified">
      <p>
        UIL-AQA achieves state-of-the-art performance across all three major long-term AQA datasets.
        Improvements are most pronounced on the Rhythmic Gymnastics (RG) and LOGO benchmarks,
        demonstrating the model’s strength in handling long temporal dependencies, uncertainty,
        and clip-level interpretability.
      </p>
    </div>

    <h3 class="title is-4">Performance Overview</h3>

    <table class="table is-striped is-hoverable is-fullwidth">
      <thead>
        <tr>
          <th>Dataset</th>
          <th>Metric</th>
          <th>Previous SOTA</th>
          <th>UIL-AQA (Ours)</th>
          <th>Improvement</th>
        </tr>
      </thead>
      <tbody>

        <!-- RG -->
        <tr>
          <td rowspan="2"><strong>Rhythmic Gymnastics (RG)</strong></td>
          <td>Avg. SRCC ↑</td>
          <td>0.842 (Inter-AQA)</td>
          <td><strong>0.858</strong></td>
          <td>+1.6%</td>
        </tr>
        <tr>
          <td>Avg. MSE ↓</td>
          <td>7.61 (Inter-AQA)</td>
          <td><strong>5.65</strong></td>
          <td>−25.7%</td>
        </tr>

        <!-- Fis-V -->
        <tr>
          <td rowspan="2"><strong>Figure Skating (Fis-V)</strong></td>
          <td>Avg. SRCC ↑</td>
          <td>0.780 (Inter-AQA)</td>
          <td><strong>0.796</strong></td>
          <td>+1.6%</td>
        </tr>
        <tr>
          <td>Avg. MSE ↓</td>
          <td>1.745 (Inter-AQA)</td>
          <td><strong>1.72</strong></td>
          <td>Comparable</td>
        </tr>

        <!-- LOGO -->
        <tr>
          <td rowspan="2"><strong>LOGO (Artistic Swimming)</strong></td>
          <td>Avg. SRCC ↑</td>
          <td>0.780 (Inter-AQA)</td>
          <td><strong>0.796</strong></td>
          <td>+2.1%</td>
        </tr>
        <tr>
          <td>Avg. R-ℓ2 ↓</td>
          <td>1.745 (Inter-AQA)</td>
          <td><strong>3.084</strong></td>
          <td>Better ranking despite small abs. error rise</td>
        </tr>

      </tbody>
    </table>

    <div class="content is-small has-text-grey">
      <p>
        Data extracted from the IJCV 2025 manuscript, including Tables 2, 3, and 4
        (RG, Fis-V, LOGO results).  
      </p>
    </div>

  </div>
</section>

<!-- Qualiative results -->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="assets/UIL-AQA_QualiativeResults.jpg" alt="UIL-AQA QualiativeResults figure">
      <h2 class="subtitle has-text-centered">
        UIL-AQA is a long-term Action Quality Assessment framework that regresses clip-level difficulty and quality while explicitly modelling judge uncertainty. A query-based Transformer decoder, an attention loss to prevent temporal skipping, and a Gaussian noise injection module together yield interpretable clip-wise scores and robust predictions across rhythmic gymnastics, figure skating, and group performance datasets.
      </h2>
    </div>
  </div>
</section>
<!-- End Qualiative results -->

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-four-fifths">

        <h2 class="title is-3">Results</h2>

        <div class="content has-text-justified">

          <p>
            UIL-AQA achieves state-of-the-art performance across three challenging long-term Action Quality Assessment benchmarks:
            Rhythmic Gymnastics (RG), Figure Skating Video (Fis-V), and LOng-form GrOup (LOGO). The model consistently improves
            ranking correlation (SRCC) while maintaining competitive or superior mean squared error performance.
          </p>

          <h3 class="title is-5">Rhythmic Gymnastics (RG)</h3>
          <ul>
            <li>Achieves the <strong>highest average SRCC</strong> across the four apparatuses.</li>
            <li>Improves upon Inter-AQA and CoFInAl by capturing fine-grained clip-level structure.</li>
            <li>Reduces MSE for most subcategories, showing improved absolute scoring accuracy.</li>
          </ul>

          <h3 class="title is-5">Figure Skating Video (Fis-V)</h3>
          <ul>
            <li>Improves both TES and PCS SRCC compared with prior state-of-the-art methods.</li>
            <li>Outperforms Inter-AQA by introducing uncertainty-aware scoring and better long-term modelling.</li>
            <li>Maintains stable MSE values, demonstrating that SRCC gains are not at the expense of error inflation.</li>
          </ul>

          <h3 class="title is-5">LOng-form GrOup (LOGO)</h3>
          <ul>
            <li>Large performance gain (<strong>+26.5% SRCC</strong> improvement vs prior methods using I3D features).</li>
            <li>With VST features, UIL-AQA achieves the <strong>highest correlation</strong> across all methods.</li>
            <li>Robust under multi-person, multi-view sequences—the most complex setting in long-term AQA.</li>
          </ul>

          <h3 class="title is-5">Why UIL-AQA Performs Better</h3>
          <ul>
            <li><strong>Attention Loss</strong> prevents temporal skipping and stabilises long-range temporal alignment.</li>
            <li><strong>Difficulty–Quality Regression</strong> provides richer supervision than single-score regression.</li>
            <li><strong>Gaussian Noise Injection</strong> improves robustness to judge bias and noisy labels.</li>
            <li><strong>Query Initialization</strong> encourages diverse temporal representations.</li>
          </ul>

          <p>
            Together, these improvements allow UIL-AQA to more accurately capture the temporal dynamics and subjective scoring
            characteristics of complex, long-duration performances.
          </p>

        </div>

      </div>
    </div>
  </div>
</section>

<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{Dong2025UILAQA,
  author    = {Xu Dong and Xinran Liu and Wanqing Li and Anthony Adeyemi{-}Ejeye and Andrew Gilbert},
  title     = {UIL-AQA: Uncertainty-aware Clip-level Interpretable Action Quality Assessment},
  journal   = {International Journal of Computer Vision},
  year      = {2025},
}
</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
